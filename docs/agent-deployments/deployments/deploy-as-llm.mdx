---
title: Deploy as LLM
sidebar_label: LLM
description: Step-by-step guide to expose your SmythOS agent as an OpenAI-compatible API using baseURL, AgentLLM key, and model string. Call /chat/completions just like with GPT.
keywords: [SmythOS, LLM deployment, OpenAI compatible, chat/completions, AgentLLM key, baseURL, agent id, version tags, API endpoint, GPT integration, OpenAI integration, OpenAI API alternative, chat completion API, OpenAI wrapper, AI agent API, SmythOS API docs]
---

# Deploy as LLM

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<InfoCallout title="Quick Steps (TL;DR)">
**1.** Deploy the agent → **Settings → Deployments**  
**2.** Toggle **LLM** ON  
**3.** Go to **Test → LLM → Keys**, create API key  
**4.** Call `/chat/completions` with `model: "agentId@dev|@prod|@1.0"`
</InfoCallout>

## When to Use LLM Deployment in SmythOS?

LLM Deployment is ideal when you want to expose your SmythOS agent as a flexible, OpenAI-compatible API. It allows any app, script, or tool that works with GPT to work easily with your own agent logic.

Use it when you need to:

- **Integrate with your backend**  
  Call your agent from Python, Node.js, or other server-side environments.

- **Power internal tools or automations**  
  Use the OpenAI-compatible API in low-code tools like Make, Zapier, or n8n.

- **Replace OpenAI with your own logic**  
  Substitute GPT with a version-controlled agent tailored to your domain.

- **Expose a stable API to others**
  Share access with devs or third-party tools using scoped API keys and versioned models.

## Key Terms for LLM Deployment

Before starting, here’s a breakdown of key terms you’ll see throughout the guide:

| Term | Where you see it | What it means | Example |
|---|---|---|---|
| **Agent name** | Studio header / list | Friendly display name. Not used in API. | Northern Lights SEO Blogger |
| **Agent ID** | In URLs & model selector | Unique ID for your agent; used in API calls. | `cmb5b4js00fmjh6333d0foxjr` |
| **Version tag** | Model dropdown | Which version of the agent to run. | `@dev`, `@prod`, `@1.0` |
| **Model string** | Request body | `agentId` + version tag. | `cmb5b4js00fmjh6333d0foxjr@prod` |
| **baseURL** | **Test → LLM → Code** | Root OpenAI‑compatible endpoint hosted by SmythOS. | `https://llm.emb-stg.smyth.ai/_openai/v1` |

<WarningCallout title="Copy from the UI">
Environments vary. Always copy the **baseURL** and **model** directly from **Test → LLM** for the agent you’re using.
</WarningCallout>

<Spacer size="md" />

## Prerequisites

Before you begin, make sure you:

- Have built and saved an agent in SmythOS Studio
- Are logged into your SmythOS account
- Know which environment you want to deploy to (`@dev`, `@prod`, or versioned)

## Step 1: Toggle On the LLM Deployment

1. Open your agent in Studio.  
2. Go to **Settings → Deployments**.  
3. Toggle **LLM** to **On**.

This exposes your agent behind an OpenAI‑compatible interface.

<Spacer size="md" />

## Step 2: Copy Your Agent’s baseURL

Click **Test** (top‑right) → **LLM** tab → **Code**.  
Copy the **Base URL**, for example:
```url
https://llm.emb-stg.smyth.ai/_openai/v1
```

**What is baseURL?**  
The root of the OpenAI‑style API that SmythOS hosts for your agent. You’ll call standard paths like `/chat/completions` against this URL.

<Spacer size="md" />

## Step 3: Select a Model Version to Use

From **Test → LLM** use the **Version** dropdown or copy the string shown in example requests:

- `agentId@dev` — current in‑development agent  
- `agentId@prod` — latest deployed agent  
- `agentId@x.y` — specific deployed version (e.g., `@1.0`)

Your final **model string** looks like: `cmb5b4js00fmjh6333d0foxjr@prod`.

<Spacer size="md" />

## Step 4: Generate an AgentLLM API Key

In **Test → LLM → Keys**:

1. Click **Create new key**.  
2. Name it (e.g., “Postman”, “Backend”).  
3. Copy the generated key (`sk‑…`) and store it securely (env var recommended).

<InfoCallout title="Security">
Treat AgentLLM keys like any production secret. Do not embed them in client‑side code.
</InfoCallout>

<Spacer size="md" />

## Step 5: Send Your First Request

All examples call the OpenAI‑compatible `/chat/completions` using your **baseURL**, **model**, and **AgentLLM** key.

<Tabs
  defaultValue="node"
  groupId="llm-examples"
  values={[
    { label: 'Node.js Example', value: 'node' },
    { label: 'Python Example', value: 'py' },
    { label: 'cURL Example', value: 'curl' },
    { label: 'Raw JSON Payload', value: 'json' }
  ]}>

  <TabItem value="node">

```js
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.SMYTHOS_AGENTLLM_KEY,
  baseURL: "https://llm.emb-stg.smyth.ai/_openai/v1",

});

const response = await openai.chat.completions.create({
  model: "cmb5b4js00fmjh6333d0foxjr@dev",
  // Available:
  // ...@dev  -> current in-development agent
  // ...@prod -> latest deployed agent
  // ...@1.0  -> specific deployed version
  messages: [{ role: "user", content: "Hello, what can you do?" }],
  stream: false,
});

console.log(response?.choices);
```
  </TabItem>

  <TabItem value="py">
```python
from openai import OpenAI
import os

client = OpenAI(
  api_key=os.getenv("SMYTHOS_AGENTLLM_KEY"),
  base_url="https://llm.emb-stg.smyth.ai/_openai/v1"
)

response = client.chat.completions.create(
  model="cmb5b4js00fmjh6333d0foxjr@dev",
  messages=[{"role": "user", "content": "Hello, what can you do?"}],
)

print(response.choices)
```
  </TabItem>

  <TabItem value="curl">

<TipCallout title="Validate Your JSON Requests">
You can use [JSONLint](https://jsonlint.com/) or Postman’s visual editor to check the structure of your request body before sending.
</TipCallout>

```bash
# Using a literal key
curl https://llm.emb-stg.smyth.ai/_openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-a937f3a5-3aee-487b-bc2c-432a287aee65" \
  -d '{
  "model": "cmb5b4js00fmjh6333d0foxjr@dev",
  "messages": [{"role":"user","content":"Hello, what can you do?"}]
}'

# Using an env var
curl https://llm.emb-stg.smyth.ai/_openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $SMYTHOS_AGENTLLM_KEY" \
  -d '{
  "model": "cmb5b4js00fmjh6333d0foxjr@dev",
  "messages": [{"role":"user","content":"Hello, what can you do?"}]
}'
```
  </TabItem>

  <TabItem value="json">
```json
{
  "model": "cmb5b4js00fmjh6333d0foxjr@dev",
  "messages": [
    { "role": "user", "content": "Hello, what can you do?" }
  ]
}
```
  </TabItem>
</Tabs>

<TipCallout title="Import into Postman Instantly">
Paste the cURL example into **Postman → Import → Raw text** and hit enter. Your request is ready to test.
</TipCallout>

<Spacer size="md" />

## Test the LLM API Endpoint

This is the simplest valid curl request you can send to your deployed agent.  
Use it with Postman, cURL, or any OpenAI-compatible client. Replace "agentId@prod" with your actual model string (like `cmb5b4js00fmjh6333d0foxjr@prod`):

```bash
curl https://YOUR_BASE_URL/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_AGENTLLM_KEY" \
  -d '{
    "model": "YOUR_AGENT_ID@prod",
    "messages": [{ "role": "user", "content": "What can you do?" }]
  }'
```


<TipCallout title="Enable Streaming Responses">
Set `"stream": true` in your request to receive responses in real time.  
This works just like OpenAI's streaming and is useful for chat UIs or progressive output.

```json
{
  "model": "agentId@prod",
  "stream": true,
  "messages": [
    { "role": "user", "content": "Tell me a story." }
  ]
}
```
</TipCallout>

## Switching Model Versions (dev, prod, or versioned)

- **@dev** — fastest iteration; reflects current draft.  
- **@prod** — latest deployed production version.  
- **@x.y** — immutable deployed version (e.g., `@1.0`).  

Switch versions by changing the `model` string without code changes elsewhere.

<InfoCallout title="Where to copy from">
Both **baseURL** and **model** appear in **Test → LLM** (Code & Version dropdown). Copy them to avoid typos and environment mix‑ups.
</InfoCallout>

<Spacer size="md" />

## After You’re Live

<SuccessCallout title="Success">
Your SmythOS LLM agent is live! 
</SuccessCallout>

- Exercise the endpoint with cURL/Postman, then integrate via any OpenAI SDK.  
- Monitor requests in **Settings → Logs**.  
- Roll out new behavior by deploying a version and updating the `@tag`.  
- Pair this API with a UI, or also **[Deploy as Chatbot](/docs/agent-deployments/deployments/deploy-as-chatbot)** / **[Deploy as ChatGPT](/docs/agent-deployments/deployments/deploy-as-chatgpt)**.

You now have a versioned, OpenAI‑compatible API powered by your SmythOS agent.

<Spacer size="md" />

## What's Next?

- [Build workflows in SmythOS](/docs/agent-studio/build-agents/building-workflows)  
- [Use chatbot with ChatGPT](/docs/agent-deployments/deployments/deploy-as-chatgpt)  
- [Expose your agent as an API](/docs/agent-deployments/deployments/deploy-as-api)

<details>
<summary><strong>Troubleshooting</strong></summary>

| Symptom | Fix |
|---|---|
| **401 Unauthorized** | Check `Authorization: Bearer {AgentLLM key}`. Regenerate under **Test → LLM → Keys**. |
| **Wrong version responding** | Verify the `model` tag (`@dev`, `@prod`, or `@x.y`). |
| **Network/URL errors** | Use the exact **baseURL** shown in **Test → LLM** for your environment. |
| **Empty/Unexpected output** | Inspect **Deployments → Logs** and confirm **LLM** is toggled on. |

</details>
