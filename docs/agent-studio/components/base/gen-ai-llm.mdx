---
title: GenAI LLM
description: Use the GenAI LLM component in SmythOS Studio to summarise, generate, extract, and classify text with large language models. Configure prompts, connect inputs, fine-tune settings, map outputs, and debug efficiently.
keywords: ["GenAI LLM", "SmythOS Studio", "LLM component", "prompt design", "OpenAI", "Claude", "Gemini", "Groq", "text generation", "summarisation", "classification", "extraction", "RAG", "debugging", "AI workflows"]
---

# GenAI LLM Component

The **GenAI LLM** component adds language skills to your agents. Write a precise prompt, select a model, connect inputs and outputs, then test and refine. Studio manages execution, security, observability, and file parsing so you can focus on building outcomes.

<InfoCallout title="When To Use GenAI LLM">
Start here when you need to **summarise, generate, extract, or classify** text. It’s the core building block for most agents and workflows.
</InfoCallout>

<Spacer size="md" />

## What You Can Build Quickly

- **Summarise**: Turn long documents into action-oriented briefs  
- **Generate**: Draft emails, replies, or outlines automatically  
- **Extract**: Pull values from text (names, dates, amounts) into JSON  
- **Classify**: Route tickets by priority, category, or sentiment  
- **Process Files**: Parse PDF or DOCX into searchable text  

<Divider />

## Step 1: Select a Model

Pick a built-in model or connect your own.

| Field | Required? | Description | Notes |
|-------|-----------|-------------|-------|
| **Model** | <Badge type="required">Yes</Badge> | The LLM that executes your prompt | Includes GPT-5 family, Claude, Gemini, Groq, and others |
| **Custom Model** | <Badge type="optional">No</Badge> | Your own hosted endpoint or API | Best for specialised domains or large context sizes |

<InfoCallout title="Context Windows">
Always leave space in the context window for both your input and the model’s output.
</InfoCallout>

<Spacer size="md" />

## Step 2: Write a Precise Prompt

The prompt tells the model what to do, the format to use, and any constraints.

<PromptCard
  title="Example Prompt"
  prompt={`You are an assistant that extracts structured insights.
From {{Attachment.text}}, create a JSON object with:
- "title": one-sentence summary
- "key_points": 3 bullet points
- "action": a next step recommendation

Return only valid JSON.`}
  tags={['LLM', 'prompt', 'extraction', 'JSON']}
/>

<TipCallout title="Prompt Guidelines">
- Be explicit about the task and expected format  
- Reference inputs like `{{Input}}` or `{{Attachment.text}}`  
- Constrain tone, length, or schema if needed  
- Break complex requests into ordered steps  
</TipCallout>

<Spacer size="md" />

## Step 3: Connect Inputs

Inputs are values you pass into the model.

| Input | Required? | Description | Notes |
|-------|-----------|-------------|-------|
| **Input** | <Badge type="required">Yes</Badge> | Main string or variable used in the prompt | Inserted as `{{Input}}` |
| **Attachment** | <Badge type="optional">No</Badge> | Files like PDF, DOCX, PNG, or JPG | Auto-converted to text and available as `Attachment.text` |

<Arcade src="https://demo.arcade.software/M2WLIzEDJbkdeJOShwhB?embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true" title="GenAI LLM Inputs | SmythOS" />

<TipCallout title="Reusable Inputs">
Rename inputs, set data types, and define defaults so your component is reusable across workflows.
</TipCallout>

<Spacer size="md" />

## Step 4: Configure Model Settings

Start with defaults. Adjust only when you need to guide behaviour.
<Tabs
  className="overflow-x-auto whitespace-nowrap"
  tabs={[
    {
      label: 'Max Tokens',
      content: (
        <>
          <p><strong>Maximum Output Tokens</strong> caps reply length and prevents cutoffs.</p>
          <ul>
            <li><strong>Short replies</strong>: 128 to 256</li>
            <li><strong>Long form</strong>: 1024 to 4096</li>
            <li><strong>Typical default</strong>: 2048 or 8192 by model</li>
          </ul>
          <TipCallout title="Sizing tip">
            Leave headroom in the context window for both input and output.
          </TipCallout>
          <PromptCard
            title="Example"
            prompt="Summarise {{Input}} in 120 to 150 words. Output markdown."
          />
        </>
      )
    },
    {
      label: 'Verbosity',
      content: (
        <>
          <p><strong>Verbosity</strong> controls how much reasoning detail appears in the output.</p>
          <ul>
            <li><strong>Low</strong>: Minimal reasoning text</li>
            <li><strong>Medium</strong>: Balanced detail</li>
            <li><strong>High</strong>: Step by step explanations</li>
          </ul>
          <InfoCallout title="Availability">GPT 5 only.</InfoCallout>
        </>
      )
    },
    {
      label: 'Reasoning',
      content: (
        <>
          <p><strong>Reasoning Effort</strong> trades speed and cost for deeper analysis.</p>
          <ul>
            <li><strong>Low</strong>: Fast and economical</li>
            <li><strong>Medium</strong>: Default balance</li>
            <li><strong>High</strong>: Better for planning and multi constraint tasks</li>
          </ul>
          <InfoCallout title="Availability">GPT 5 only.</InfoCallout>
          <TipCallout title="Good uses">
            Multi step planning. Validated extraction. Non trivial transformations.
          </TipCallout>
        </>
      )
    },
    {
      label: 'Top P',
      content: (
        <>
          <p><strong>Top P</strong> samples from the smallest set of tokens whose probabilities sum to P.</p>
          <ul>
            <li><strong>0.7 to 0.9</strong>: Typical variety</li>
            <li><strong>0.3 to 0.6</strong>: Tighter and more focused</li>
          </ul>
          <TipCallout>Prefer one sampling control at a time. Use Top P if Temperature is unavailable.</TipCallout>
        </>
      )
    },
    {
      label: 'Top K',
      content: (
        <>
          <p><strong>Top K</strong> restricts sampling to the K most likely tokens.</p>
          <ul>
            <li><strong>K 20 to 50</strong>: Safer and consistent</li>
            <li><strong>Higher K</strong>: More diverse with higher drift risk</li>
          </ul>
        </>
      )
    },
    {
      label: 'Stop Seq',
      content: (
        <>
          <p><strong>Stop Sequences</strong> define strings where generation should end.</p>
          <ul>
            <li>Use delimiters like <code>\n\n###</code> or <code>END_JSON</code></li>
            <li>Helpful for multi section outputs and extractors</li>
          </ul>
          <InfoCallout title="Availability">Some models. Often not supported on OpenAI or Anthropic.</InfoCallout>
          <PromptCard
            title="Example"
            prompt={[
              'Extract a JSON object with keys title and summary from {{Input}}.',
              'Only output JSON.',
              'Stop when you reach END_JSON.',
              '{ "title": "", "summary": "" }',
              'END_JSON'
            ].join('\n')}
          />
        </>
      )
    },
    {
      label: 'Penalty',
      content: (
        <>
          <p><strong>Penalty Settings</strong> reduce repetition and encourage novelty.</p>
          <ul>
            <li><strong>Frequency Penalty</strong>: discourages repeated tokens</li>
            <li><strong>Presence Penalty</strong>: encourages new topics compared to the input</li>
          </ul>
          <TipCallout>Adjust one at a time for predictable results.</TipCallout>
        </>
      )
    },
    {
      label: 'Ctx Window',
      content: (
        <>
          <p><strong>Context Window</strong> includes recent conversation or memory in the request.</p>
          <ul>
            <li>Include only relevant turns to save tokens</li>
            <li>Summarise long history when needed</li>
          </ul>
          <PromptCard
            title="Example"
            prompt="Using the last 3 user turns only, summarise the decision and next action."
          />
        </>
      )
    },
    {
      label: 'Agent Prompt',
      content: (
        <>
          <p><strong>Agent System Prompt</strong> applies your agent rules and tone to this call.</p>
          <ul>
            <li>Keep it short and explicit</li>
            <li>Include non negotiable rules and style</li>
          </ul>
          <PromptCard
            title="Template"
            prompt={[
              'You are a helpful agent.',
              'Follow the schema.',
              'If unsure, ask for the missing field.',
              'Always return valid JSON.'
            ].join('\n')}
          />
        </>
      )
    },
    {
      label: 'Passthrough',
      content: (
        <>
          <p><strong>Passthrough Mode</strong> returns raw model output for custom parsing.</p>
          <ul>
            <li><strong>Off</strong>: Studio streams and formats output</li>
            <li><strong>On</strong>: You own parsing and rendering</li>
          </ul>
          <TipCallout>Use when a downstream step validates or post processes the text.</TipCallout>
        </>
      )
    },
    {
      label: 'Web Search',
      content: (
        <>
          <p><strong>Use Web Search</strong> lets the model fetch current facts when accuracy depends on fresh data.</p>
          <ul>
            <li>Enable for news, prices, schedules</li>
            <li>Disable for static knowledge tasks</li>
          </ul>
          <TipCallout>Pair with lower variability for reliable citations.</TipCallout>
        </>
      )
    },
    {
      label: 'Temperature',
      content: (
        <>
          <p><strong>Temperature</strong> adjusts randomness in token selection.</p>
          <ul>
            <li><strong>0.0 to 0.2</strong>: Deterministic for facts and extraction</li>
            <li><strong>0.3 to 0.6</strong>: Balanced for assistants</li>
            <li><strong>0.7 to 1.0</strong>: Creative for ideation</li>
          </ul>
          <InfoCallout title="Availability">Some models. Often not supported on OpenAI or Anthropic in this component.</InfoCallout>
        </>
      )
    },
    {
      label: 'Recipes',
      content: (
        <>
          <p><strong>Sampling Recipes</strong> to start fast.</p>
          <ul>
            <li><strong>Deterministic QA</strong>: Top P 0.4, Frequency Penalty 0.2</li>
            <li><strong>Creative Copy</strong>: Top P 0.9, no penalties</li>
            <li><strong>Structured Extract</strong>: Stop Sequences set, Frequency Penalty 0.3</li>
          </ul>
        </>
      )
    }
  ]}
/>

<Spacer size="md" />

### Quick Reference

| Setting | What It Controls | OpenAI | Anthropic | Other Providers |
|---------|------------------|--------|-----------|-----------------|
| **Maximum Output Tokens** | Caps how many tokens the model can generate in one reply | <Tick /> | <Tick /> | <Tick /> |
| **Verbosity** | Detail level in reasoning output | GPT-5 only | <Cross /> | <Cross /> |
| **Reasoning Effort** | Trade-off between deeper reasoning and speed | GPT-5 only | <Cross /> |<Cross />|
| **Passthrough** | Returns raw, unformatted output | <Tick /> | <Tick /> | <Tick /> |
| **Use Agent System Prompt** | Applies global system instructions consistently | <Tick /> | <Tick /> | <Tick /> |
| **Use Context Window** | Includes conversation history in requests | <Tick /> | <Tick /> | <Tick /> |
| **Use Web Search** | Lets the model fetch real-time facts | <Tick /> | <Tick /> | <Tick /> |
| **Top P** | Probability mass sampling for variety | <Tick /> | <Tick /> | <Tick /> |
| **Top K** | Restricts sampling to top K tokens | <Tick /> | <Tick /> | <Tick /> |
| **Temperature** | Controls randomness and creativity | <Cross /> | <Cross /> | <Tick /> |
| **Stop Sequences** | Defines strings where generation should end | <Cross /> | <Tick /> |  <Tick /> |

<Arcade src="https://demo.arcade.software/As1gjawhDBX6BV3Y7CQz?embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true" title="GenAI LLM Settings | SmythOS" />

<InfoCallout title="Quick Presets">
**Extractor**: Max Tokens 256, Stop Sequences set, Frequency Penalty 0.3  
**Summariser**: Max Tokens 512–1024, Temperature 0.3, Top P 1.0  
**Brainstormer**: Max Tokens 512, Temperature 0.8, Top P 0.9  
</InfoCallout>

<Spacer size="md" />

## Step 5: Define Outputs

Expose the model’s reply and map fields for downstream use.

| Output | Required? | Description | Example |
|--------|-----------|-------------|---------|
| **Reply** | <Badge type="required">Yes</Badge> | Main model output | Paragraph, list, or JSON text |
| **Custom Output** | <Badge type="optional">No</Badge> | Extracted fields from the reply | `Reply.summary`, `Reply.json.customer_id` |

<PromptCard
  title="Custom Output Mapping"
  prompt={`[
  { "Name": "title", "Expression": "Reply.title" },
  { "Name": "summary", "Expression": "Reply.summary", "Format": "markdown" }
]`}
/>

<InfoCallout title="Formatting Options">
Custom outputs can be returned as `text`, `markdown`, `html`, or `json`, depending on downstream requirements.
</InfoCallout>

<Spacer size="md" />

<Divider />

## Before You Go Live

You are close. Run a quick loop to make sure this component behaves the way you expect, then [deploy](/docs/agent-deployments/quickstart) it and keep an eye on it.

<InfoCallout title="Quick Play Test">
- Open **Debug Mode** and run 3 real samples from your use case  
- Read the **raw response** and confirm your **Custom Outputs** render correctly  
- Try one bad input on purpose to see how the component fails and recovers  
</InfoCallout>

<TipCallout title="Review These Things">
- Cutoffs → raise **Max Tokens** or trim the input  
- Wobbly tone or drift → lower **Top P** or add a **Frequency Penalty**  
- Messy format → add a clear schema in the prompt and use **Stop Sequences** if your model supports them  
- Extra tokens spent on history → reduce **Context Window** or summarise prior turns  
</TipCallout>

<PromptCard
  title="Test Your Component"
  prompt={`Using {{Input}} or {{Attachment.text}}, return only this JSON:
{
  "title": "<one sentence>",
  "key_points": ["<point 1>", "<point 2>", "<point 3>"],
  "action": "<one next step>"
}
If a field is missing, output an empty string instead of guessing.`}
/>

### Ship And Watch It

- Connect to the [Code Component](/docs/agent-studio/components/advanced/code) for validation or post processing  
- Add to your workflow and follow the [Deploying Agents](/docs/agent-deployments/overview) guide  
- Keep an eye on logs and usage in **Observability**. Look for spikes in token count, latency jumps, or empty fields

<InfoCallout title="Green Flags To Publish">
- Sample inputs produce the exact shape you asked for  
- Custom Outputs render as the right types: text, markdown, html, or json  
- Guardrails for length, format, and repetition are in place  
- You have at least one real failure case tested and handled  
</InfoCallout>

<TipCallout title="When To Revisit Settings">
If your data changes or latency targets tighten, revisit **Max Tokens**, **Top P**, and **Reasoning Effort**. Small changes can reduce cost without hurting quality.
</TipCallout>

<Spacer size="lg" /> 

## What's Next 

- Review the [Prompt Guide](/docs/agent-weaver/prompt-guide) for proven techniques 
- Use the [Debugging Guide](/docs/agent-studio/build-agents/debugging) for deeper techniques
- Explore [Data Spaces](/docs/agent-collaboration/spaces/) if you need persistent knowledge