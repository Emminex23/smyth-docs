---
title: GenAI LLM
description: Generate AI responses using prompt-driven input, with built-in models or custom APIs.
keywords: [GenAI, LLM, language model, prompt, OpenAI, outputs, attachments]
toc_max_heading_level: 2
---

# GenAI LLM Component

Use the **GenAI LLM** component to generate dynamic responses using a prompt and an LLM like OpenAI or Anthropic. It's perfect for summaries, content generation, extraction, and more.

<InfoCallout title="Why this matters">
With GenAI LLM, you can transform inputs like text or files into structured or styled output.  
No custom code needed. You only need a prompt, a model, and the right inputs.
</InfoCallout>

<Spacer size="md" />

## What You’ll Configure

- [Model Selection](#step-1-choose-a-model)
- [Prompt Setup](#step-2-define-the-prompt)
- [Input Binding](#step-3-add-inputs)
- [Advanced Parameters](#step-4-configure-advanced-settings)
- [Output Mapping](#step-5-define-outputs)
- [Testing & Debug](#step-6-debug-and-preview)
- [Best Practices](#best-practices)
- [Troubleshooting Tips](#troubleshooting-tips)
- [What to Try Next](#what-to-try-next)

<Spacer size="md" />

<InfoCallout title="Model setup">
SmythOS includes default models (e.g. OpenAI GPT).  
You can also connect custom providers like Claude or Gemini.  
See [Billing Management →](/docs/account-management/billing-management) for cost and usage.
</InfoCallout>

<Spacer size="md" />

## Step 1: Choose a Model

| Field            | Required? | Description                                     | Tips |
|------------------|-----------|-------------------------------------------------|------|
| **Model**        | <Badge type="required">Yes</Badge> | The LLM used for generation or classification | By default you will see **GPT-5 Mini (SmythOS 400k)**. You can also choose GPT-5, GPT-5 Nano, GPT-5 Chat, Claude Sonnet 3.7, Gemini 2.5 Pro, Sonar, or other supported engines. |
| **Custom Model** | <Badge type="optional">No</Badge> | Use your own hosted model or external API | Connect to OpenAI, Claude, Google AI, Groq, or any endpoint compatible with your workflow. |

<InfoCallout title="Token limits"> 
Default GPT-5 family models have token windows up to 400k tokens (shown next to each model name). If you need to handle even larger inputs or maintain more conversation history, use a custom model with extended limits. 
</InfoCallout>

<Spacer size="md" />

## Step 2: Define the Prompt

<PromptCard
  title="Basic Prompt"
  prompt="Summarize {{Input}} into one paragraph."
  tags={['LLM', 'prompt', 'template']}
/>

<TipCallout title="Prompt inputs">
Use `{{Input}}`, `{{input.email}}`, or even `{{Attachment.text}}` in your prompt.  
Files like PDFs or DOCX will be auto-processed into text where possible.
</TipCallout>

<Spacer size="md" />

## Step 3: Add Inputs

| Input Name    | Required? | Description                          | Notes |
|---------------|-----------|--------------------------------------|-------|
| **Input**     | <Badge type="required">Yes</Badge> | Main prompt content | Injected as `{{Input}}` |
| **Attachment**| <Badge type="optional">No</Badge> | Files or images (PDF, DOCX, JPG, etc.) | Converted to text for model input |

<TipCallout>
You can rename inputs, assign types (string, file, etc.), and set defaults or make them optional.
</TipCallout>

<Arcade
  src="https://demo.arcade.software/M2WLIzEDJbkdeJOShwhB?embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true"
  title="GenAI LLM Inputs | SmythOS"
/>

<Spacer size="md" />

## Step 4: Configure Advanced Settings

<Tabs
  tabs={[
    {
      label: 'Temperature',
      content: (
        <>
          <p><strong>Temperature</strong> controls randomness in model output.</p>
          <p>Lower = focused. Higher = creative.</p>
        </>
      )
    },
{
      label: 'Max Tokens',
      content: (
        <>
          <p><strong>Max Tokens</strong> limits the output length. This is the maximum number of tokens the model can generate in a single response.</p>
          <ul>
            <li><strong>Default Value:</strong> <strong>8192 (8k tokens)</strong></li>
            <li><strong>Purpose:</strong> Controls the size of the model’s response output</li>
            <li><strong>Applies to:</strong> All supported AI models</li>
            <li><strong>Configuration:</strong>
              <ul>
                <li>Settings persist across browser sessions</li>
                <li>Can be modified per component instance</li>
                <li>Reset button returns to default <strong>8192</strong> value</li>
              </ul>
            </li>
          </ul>
          <p><strong>Note:</strong> This default provides a good balance between response completeness and processing efficiency.</p>
          <TipCallout title="Pro Tip">
            The <strong>8k default</strong> works well for most use cases. Adjust only if you need longer responses or want to reduce token usage for cost optimization.
          </TipCallout>
        </>
      )
    },
    {
      label: 'Top P',
      content: (
        <>
          <p><strong>Top P</strong> is an alternative to Temperature that affects token-selection diversity.</p>
        </>
      )
    },
    {
      label: 'Stop Sequences',
      content: (
        <>
          <p><strong>Stop Sequences</strong> let you define when the model should stop generating.</p>
        </>
      )
    },
    {
      label: 'Penalty Settings',
      content: (
        <>
          <p><strong>Frequency Penalty</strong> discourages repeated tokens.</p>
          <p><strong>Presence Penalty</strong> lowers repetition of ideas or topics.</p>
          <TipCallout>Use only one at a time for best results.</TipCallout>
        </>
      )
    },
        {
      label: 'Verbosity',
      content: (
        <>
          <p><strong>Verbosity</strong> controls how much detail the model includes in its reasoning output.</p>
          <ul>
            <li><strong>Low:</strong> Concise reasoning steps.</li>
            <li><strong>Medium (default):</strong> Balanced detail.</li>
            <li><strong>High:</strong> More explicit, step-by-step reasoning.</li>
          </ul>
          <p><strong>Applies to:</strong> GPT-5 models only.</p>
        </>
      )
    },
    {
      label: 'Passthrough Mode',
      content: (
        <>
          <p><strong>Passthrough Mode</strong> gives you full control of LLM output without formatting.</p>
          <ul>
            <li><strong>Off:</strong> Output is parsed and streamed automatically</li>
            <li><strong>On:</strong> You decide how it’s displayed or used</li>
          </ul>
        </>
      )
    },
    {
      label: 'Agent System Prompt',
      content: (
        <>
          <p><strong>Agent System Prompt</strong> injects your agent’s top-level instructions or persona into this LLM call.</p>
          <p>Enable when you need consistent tone or behavior across the agent.</p>
          <TipCallout>Handy for ensuring any global “You are a helpful assistant” or custom instructions are applied here.</TipCallout>
        </>
      )
    },
    {
      label: 'Context Window',
      content: (
        <>
          <p><strong>Context Window</strong> includes recent conversation history or memory in this LLM request.</p>
          <p>Enable to give the model visibility of prior messages, so it can maintain continuity.</p>
          <TipCallout>Ideal for multi-turn dialogues where the model needs context from earlier exchanges.</TipCallout>
        </>
      )
    },
    {
      label: 'Reasoning Effort',
      content: (
        <>
          <p><strong>Reasoning Effort</strong> sets how much processing the model invests in reasoning.</p>
          <ul>
            <li><strong>Low:</strong> Faster responses, fewer reasoning tokens used.</li>
            <li><strong>Medium (default):</strong> Balanced speed and reasoning depth.</li>
            <li><strong>High:</strong> More in-depth reasoning, potentially longer responses.</li>
          </ul>
          <p><strong>Applies to:</strong> GPT-5 and GPT Open Source (Groq) models.</p>
          <TipCallout title="Why adjust this?">
            Lower effort is useful for speed and cost savings. Higher effort can improve reasoning quality for complex problems.
          </TipCallout>
        </>
      )
    }
  ]}
/>

<Arcade
  src="https://demo.arcade.software/As1gjawhDBX6BV3Y7CQz?embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true"
  title="GenAI LLM Settings | SmythOS"
/>

<Spacer size="md" />

## Step 5: Define Outputs

| Field           | Required? | Description                        | Notes |
|----------------|-----------|------------------------------------|-------|
| **Reply**       | <Badge type="required">Yes</Badge> | The model's main output         | Default output |
| **Custom Output** | <Badge type="optional">No</Badge> | Extract fields using expressions | `Reply.title`, `Reply.summary`, etc. |

```json
{
  "Name": "summary",
  "Expression": "Reply.summary",
  "Format": "markdown",
  "Description": "The generated summary"
}
```

<TipCallout title="Format values">
Set `Format` to `text`, `markdown`, `html`, or `json` for downstream handling.
</TipCallout>

<InfoCallout title="Validation">
SmythOS won’t validate your Custom Output expressions.  
Make sure field names like `Reply.title` exist.
</InfoCallout>

<Arcade
  src="https://demo.arcade.software/I35gGim7BajWDMmGkhBQ?embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true"
  title="GenAI LLM Outputs | SmythOS"
/>

<Spacer size="md" />

## Step 6: Debug and Preview

<InfoCallout title="Live preview">
Use **Debug** to run your prompt with real or sample input.  
You’ll see live results and can tweak output mappings right away.
</InfoCallout>

**Example Prompt Input:**  
“Write a short article about the Sakura tree.”

**Example Custom Output Mapping:**

```json
[
  { "Name": "title", "Expression": "Reply.title" },
  { "Name": "content", "Expression": "Reply.body" },
  { "Name": "keywords", "Expression": "Reply.keywords", "Format": "json" }
]
```

<Spacer size="md" />

## Best Practices

- Use specific prompts: "List 3 key points from..." is better than "Summarize"
- Format custom outputs if needed (`text`, `html`, `json`)
- Use mock inputs in Debug to test multiple prompt paths
- Avoid putting complex logic inside a prompt — let the model generate clean data
- Use `Passthrough Mode` for total control over rendering or streaming
- Use Retry + Condition blocks to handle failed outputs or empty results

<Spacer size="md" />

## Troubleshooting Tips

<InfoCallout title="If your prompt isn't working...">
- Confirm that your prompt references actual inputs (like `{{Input}}`)  
- Check for typos in custom output expressions (e.g., `Reply.summary` vs `Reply.summray`)  
- Make sure your model isn’t hitting token limits — reduce long attachments or prompts  
- If file inputs aren’t being read, verify they are processed into `Attachment.text`  
- For empty outputs, try lowering **Temperature** and simplifying your prompt  
- Run in **Debug Mode** and inspect full raw output to fine-tune your extraction fields
</InfoCallout>

<Spacer size="md" />
## What to Try Next

- Combine **GenAI LLM** with **[Agent Skill](/docs/agent-studio/components/base/agent-skill)** to let users submit prompts naturally
- Pipe GenAI output into **[RAG Remember](/docs/agent-studio/components/rag-data/rag-remember)** to store facts for reuse
- Use **[Code Component](/docs/agent-studio/components/advanced/code)** downstream to transform, filter, or validate replies

<TipCallout>
Looking for good prompt patterns? Visit our [Prompt Guide →](/docs/agent-weaver/prompt-guide).
</TipCallout>